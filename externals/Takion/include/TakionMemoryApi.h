// TakionMemory.h : main header file for the TakionMemory DLL
//

#pragma once

#ifndef __AFXWIN_H__
	#error "include 'stdafx.h' before including this file for PCH"
#endif

#ifdef TM_EXPORTS
#define TM_API __declspec(dllexport)
#else
#define TM_API __declspec(dllimport)
#endif

#ifndef _DEBUG
#define NED_MALLOC
#endif

const char* const TakionMemoryHeaderVersion = "1.0.0.169";

#define NO_NED_NAMESPACE

#ifdef __cplusplus
 #if !defined(NO_NED_NAMESPACE)
namespace nedalloc {
 #else
extern "C"
{
 #endif
 #define THROWSPEC throw()
#else
 #define THROWSPEC
#endif

const char* WINAPI TM_GetModuleVersionStr();
unsigned __int64 WINAPI TM_GetModuleVersionNum();
const char* WINAPI TM_GetHeaderVersion();
unsigned __int64 WINAPI TM_GetHeaderModuleVersionNum();
const char* WINAPI TM_GetFilePathAndName();
const char* WINAPI TM_GetPlatform();
const char* WINAPI TM_GetFileDescription();
void WINAPI TM_GetDllBuildDescription(std::string& buildStr);
bool WINAPI TM_IsNedMallocEnabled();

/* nedalloc, an alternative malloc implementation for multiple threads without
lock contention based on dlmalloc v2.8.3. (C) 2005 Niall Douglas

Boost Software License - Version 1.0 - August 17th, 2003

Permission is hereby granted, free of charge, to any person or organization
obtaining a copy of the software and accompanying documentation covered by
this license (the "Software") to use, reproduce, display, distribute,
execute, and transmit the Software, and to prepare derivative works of the
Software, and to permit third-parties to whom the Software is furnished to
do so, all subject to the following:

The copyright notices in the Software and this entire statement, including
the above license grant, this restriction and the following disclaimer,
must be included in all copies of the Software, in whole or in part, and
all derivative works of the Software, unless such copies or derivative
works are solely in the form of machine-executable object code generated by
a source language processor.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT
SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE
FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,
ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
DEALINGS IN THE SOFTWARE.
*/

/* See malloc.c.h for what each function does.

REPLACE_SYSTEM_ALLOCATOR on POSIX causes nedalloc's functions to be called
malloc, free etc. instead of nedmalloc, nedfree etc. You may or may not want
this. On Windows it causes nedmalloc to patch all loaded DLLs and binaries
to replace usage of the system allocator.

NO_NED_NAMESPACE prevents the functions from being defined in the nedalloc
namespace when in C++ (uses the global namespace instead).

NEDMALLOCEXTSPEC can be defined to be __declspec(dllexport) or
__attribute__ ((visibility("default"))) or whatever you like. It defaults
to extern unless NEDMALLOC_DLL_EXPORTS is set as it would be when building
nedmalloc.dll.

USE_LOCKS can be 2 if you want to define your own MLOCK_T, INITIAL_LOCK,
ACQUIRE_LOCK, RELEASE_LOCK, TRY_LOCK, IS_LOCKED and NULL_LOCK_INITIALIZER.

NEDMALLOC_DEBUG can be defined to cause DEBUG to be set differently for nedmalloc
than for the rest of the build. Remember to set NDEBUG to disable all assertion
checking too.

USE_MAGIC_HEADERS causes nedalloc to allocate an extra three sizeof(size_t)
to each block. nedpfree() and nedprealloc() can then automagically know when
to free a system allocated block. Enabling this typically adds 20-50% to
application memory usage.

ENABLE_TOLERANT_NEDMALLOC is automatically turned on if REPLACE_SYSTEM_ALLOCATOR
is set or the Windows DLL is being built. This causes nedmalloc to detect when a
system allocator block is passed to it and to handle it appropriately. Note that
without USE_MAGIC_HEADERS there is a very tiny chance that nedmalloc will segfault
on non-Windows builds (it uses Win32 SEH to trap segfaults on Windows and there
is no comparable system on POSIX).

USE_ALLOCATOR can be one of these settings (it defaults to 1):
  0: System allocator (nedmalloc now simply acts as a threadcache).
     WARNING: Intended for DEBUG USE ONLY - not all functions work correctly.
  1: dlmalloc

ENABLE_LARGE_PAGES enables support for requesting memory from the system in large
(typically >=2Mb) pages if the host OS supports this. These occupy just a single
TLB entry and can significantly improve performance in large working set applications.

ENABLE_FAST_HEAP_DETECTION enables special logic to detect blocks allocated
by the system heap. This avoids 1.5%-2% overhead when checking for non-nedmalloc
blocks, but it assumes that the NT and glibc heaps function in a very specific
fashion which may not hold true across OS upgrades.
*/

#ifndef EXTSPEC
 #define EXTSPEC TM_API
#endif

#define MALLOCATTR
#define NO_NED_NAMESPACE
#define NO_MALLINFO

//#define REPLACE_SYSTEM_ALLOCATOR
/*
#if defined(_MSC_VER) && _MSC_VER>=1400
 #define MALLOCATTR __declspec(restrict)
#endif
#ifdef __GNUC__
 #define MALLOCATTR __attribute__ ((malloc))
#endif
#ifndef MALLOCATTR
 #define MALLOCATTR
#endif
*/
#ifdef REPLACE_SYSTEM_ALLOCATOR
 #define nedmalloc               malloc
 #define nedcalloc               calloc
 #define nedrealloc              realloc
 #define nedfree                 free
 #define nedmemalign             memalign
 #define nedmallinfo             mallinfo
 #define nedmallopt              mallopt
 #define nedmalloc_trim          malloc_trim
 #define nedmalloc_stats         malloc_stats
 #define nedmalloc_footprint     malloc_footprint
 #define nedindependent_calloc   independent_calloc
 #define nedindependent_comalloc independent_comalloc
 #ifdef _MSC_VER
  #define nedblksize              _msize
 #endif
#endif
/*
#ifndef NO_MALLINFO
#define NO_MALLINFO 0
#endif

#if !NO_MALLINFO
struct mallinfo;
#endif
*/

/* These are the global functions */

/* Gets the usable size of an allocated block. Note this will always be bigger than what was
asked for due to rounding etc.
*/
EXTSPEC size_t nedblksize(void *mem) THROWSPEC;

EXTSPEC void nedsetvalue(void *v) THROWSPEC;

EXTSPEC MALLOCATTR void * nedmalloc(size_t size) THROWSPEC;
EXTSPEC MALLOCATTR void * nedcalloc(size_t no, size_t size) THROWSPEC;
EXTSPEC MALLOCATTR void * nedrealloc(void *mem, size_t size) THROWSPEC;
EXTSPEC void   nedfree(void *mem) THROWSPEC;
EXTSPEC MALLOCATTR void * nedmemalign(size_t alignment, size_t bytes) THROWSPEC;
#ifndef NO_MALLINFO
EXTSPEC struct mallinfo nedmallinfo(void) THROWSPEC;
#endif
EXTSPEC int    nedmallopt(int parno, int value) THROWSPEC;
EXTSPEC int    nedmalloc_trim(size_t pad) THROWSPEC;
EXTSPEC void   nedmalloc_stats(void) THROWSPEC;
EXTSPEC size_t nedmalloc_footprint(void) THROWSPEC;
EXTSPEC MALLOCATTR void **nedindependent_calloc(size_t elemsno, size_t elemsize, void **chunks) THROWSPEC;
EXTSPEC MALLOCATTR void **nedindependent_comalloc(size_t elems, size_t *sizes, void **chunks) THROWSPEC;

/* These are the pool functions */
struct nedpool_t;
typedef struct nedpool_t nedpool;

/* Creates a memory pool for use with the nedp* functions below.
Capacity is how much to allocate immediately (if you know you'll be allocating a lot
of memory very soon) which you can leave at zero. Threads specifies how many threads
will *normally* be accessing the pool concurrently. Setting this to zero means it
extends on demand, but be careful of this as it can rapidly consume system resources
where bursts of concurrent threads use a pool at once.
*/
EXTSPEC MALLOCATTR nedpool *nedcreatepool(size_t capacity, int threads) THROWSPEC;

/* Destroys a memory pool previously created by nedcreatepool().
*/
EXTSPEC void neddestroypool(nedpool *p) THROWSPEC;

/* Sets a value to be associated with a pool. You can retrieve this value by passing
any memory block allocated from that pool.
*/
EXTSPEC void nedpsetvalue(nedpool *p, void *v) THROWSPEC;
/* Gets a previously set value using nedpsetvalue() or zero if memory is unknown.
Optionally can also retrieve pool.
*/
EXTSPEC void *nedgetvalue(nedpool **p, void *mem) THROWSPEC;

/* Disables the thread cache for the calling thread, returning any existing cache
data to the central pool.
*/
EXTSPEC void neddisablethreadcache(nedpool *p) THROWSPEC;

EXTSPEC MALLOCATTR void * nedpmalloc(nedpool *p, size_t size) THROWSPEC;
EXTSPEC MALLOCATTR void * nedpcalloc(nedpool *p, size_t no, size_t size) THROWSPEC;
EXTSPEC MALLOCATTR void * nedprealloc(nedpool *p, void *mem, size_t size) THROWSPEC;
EXTSPEC void   nedpfree(nedpool *p, void *mem) THROWSPEC;
EXTSPEC MALLOCATTR void * nedpmemalign(nedpool *p, size_t alignment, size_t bytes) THROWSPEC;
#ifndef NO_MALLINFO
EXTSPEC struct mallinfo nedpmallinfo(nedpool *p) THROWSPEC;
#endif
EXTSPEC int    nedpmallopt(nedpool *p, int parno, int value) THROWSPEC;
EXTSPEC int    nedpmalloc_trim(nedpool *p, size_t pad) THROWSPEC;
EXTSPEC void   nedpmalloc_stats(nedpool *p) THROWSPEC;
EXTSPEC size_t nedpmalloc_footprint(nedpool *p) THROWSPEC;
EXTSPEC MALLOCATTR void **nedpindependent_calloc(nedpool *p, size_t elemsno, size_t elemsize, void **chunks) THROWSPEC;
EXTSPEC MALLOCATTR void **nedpindependent_comalloc(nedpool *p, size_t elems, size_t *sizes, void **chunks) THROWSPEC;

//#undef MALLOCATTR
//#undef EXTSPEC

#ifdef __cplusplus
}
#endif

#ifdef NED_MALLOC\

#define DECLARE_NED_NEW void* operator new(size_t s)\
{\
	return nedmalloc(s);\
}\
void operator delete(void* p)\
{\
	nedfree(p);\
}\
void* operator new(size_t nSize, LPCSTR lpszFileName, int nLine)\
{\
	return nedmalloc(nSize);\
}\
void operator delete(void* p, LPCSTR lpszFileName, int nLine)\
{\
	nedfree(p);\
}

#else

#define DECLARE_NED_NEW

#endif
/*
#elif _DEBUG

#define DECLARE_NED_NEW void* operator new(size_t s)\
{\
	return ::operator new(s);\
}\
void operator delete(void* p)\
{\
	::operator delete(p);\
}\
void* operator new(size_t nSize, LPCSTR lpszFileName, int nLine)\
{\
	return ::operator new(nSize, lpszFileName, nLine);\
}\
void operator delete(void* p, LPCSTR lpszFileName, int nLine)\
{\
	::operator delete(p, lpszFileName, nLine);\
}\

#else

#define DECLARE_NED_NEW void* operator new(size_t s)\
{\
	return ::operator new(s);\
}\
void operator delete(void* p)\
{\
	::operator delete(p);\
}

#endif
*/
/*
#ifdef _DEBUG
#define ALLOCATOR_STATS
#endif
*/

/*
#ifdef ALLOCATOR_STATS
class TU_API MemoryPoolStatistics
{
public:
	MemoryPoolStatistics()
		: m_locksAttempts( 0 )
		, m_locksSuccesses( 0 )
		, m_allocations( 0 )
		, m_frees( 0 )
		, m_currentlyAllocated( 0 )
		, m_commitedSize( 0 )
	{}

	unsigned long	m_locksAttempts;
	unsigned long	m_locksSuccesses;
	unsigned long	m_allocations;
	unsigned long	m_frees;
	unsigned long	m_currentlyAllocated;
	size_t			m_commitedSize;
};
#endif

template<typename T>
class FixedSizeMemoryPool
{
public:
    enum { init_size = 10 };
	
	typedef std::list<char*> PageList;
	
	FixedSizeMemoryPool()
		: m_pFree( NULL )
		, m_allocSize( sizeof(T) )
		, m_commitedSize( 0 )
	{
		Initialize();
	}

	~FixedSizeMemoryPool()
	{
		PageList::iterator itend = m_pageList.end();
		for(PageList::iterator it = m_pageList.begin(); it != itend; ++it)
		{
			kill(*it);
		}
//		std::for_each( m_pageList.begin(), m_pageList.end(), kill );
	}

	void Initialize()
	{
		SYSTEM_INFO si;
		GetSystemInfo( &si );
		m_allocationGranularity = si.dwAllocationGranularity;

		if( m_allocSize < sizeof(link) )
			m_allocSize = sizeof(link);

		if( m_allocationGranularity < m_allocSize )
		{
			m_growSize		= ( m_allocSize % m_allocationGranularity ) ? ( m_allocSize / m_allocationGranularity + 1 ) * m_allocationGranularity : m_allocSize;
			m_unitsPerGrow	= 1;

			if( m_growSize > m_allocationGranularity * m_initSize )
			{
				m_initSize = 1;
			}
			else
			{
				m_initSize = ( init_size * m_allocationGranularity ) / m_growSize;
			}
		}
		else
		{
			m_growSize		= m_allocationGranularity;
			m_unitsPerGrow	= m_growSize / m_allocSize;
			m_initSize		= init_size;
		}

		if( m_initSize > 0 )
		{
			link* pLastLink = NULL;
			for( unsigned int n = 0; n < m_initSize; ++n )
			{
				link* pLink = (link*)( VirtualAlloc( NULL, m_growSize, MEM_COMMIT, PAGE_READWRITE ) );
				if( pLink )
				{
					m_pageList.push_back((char*)pLink);
#ifdef ALLOCATOR_STATS
					++m_stats.m_locksAttempts;
					BOOL bResult = VirtualLock(pLink, m_growSize);
					if( bResult )
					{
						++m_stats.m_locksSuccesses;
					}

					m_stats.m_commitedSize = m_commitedSize += m_growSize;
#else
					VirtualLock(pLink, m_growSize);
					m_commitedSize += m_growSize;
#endif
					if( pLastLink )
					{
						pLastLink->m_pNext = pLink;
					}

					for( size_t i = 1; i < m_unitsPerGrow; ++i)
						pLink = pLink->m_pNext = (link*)((char*)(pLink) + m_allocSize);

					pLastLink = pLink;
					pLastLink->m_pNext = NULL;
				}
				else
				{
					break;
				}
			}

			if( m_commitedSize )
			{
				m_pFree = (link*)*m_pageList.begin();
			}
		}
	}

    T* Allocate()
    {
        link* pLink = m_pFree;
        if( NULL == pLink ) 
		{
			Grow();
		}

		T* ret = (T*)( m_pFree );

		if( m_pFree )
		{
			m_pFree = m_pFree->m_pNext;
#ifdef ALLOCATOR_STATS
			++m_stats.m_allocations;
			++m_stats.m_currentlyAllocated;
#endif
		}

        return ret;
    }

	void Deallocate( void* p )
    {
        link* pLink = m_pFree;
        m_pFree = (link*)p;
		m_pFree->m_pNext = pLink;
#ifdef ALLOCATOR_STATS
		++m_stats.m_frees;
		--m_stats.m_currentlyAllocated;
#endif
    }

#ifdef ALLOCATOR_STATS
	// Statistics
	const MemoryPoolStatistics& GetStatistics() const{return m_stats;}
#endif

private:
	struct link
	{
		link* m_pNext;
	};

	void Grow()
	{
		link* pLink = (link*)VirtualAlloc(NULL, m_growSize, MEM_COMMIT, PAGE_READWRITE);

		if( pLink )
		{
			m_pageList.push_back((char*)pLink);

			link* pOldFree = m_pFree;
			m_pFree = pLink;

#ifdef ALLOCATOR_STATS
			++m_stats.m_locksAttempts;
			if(VirtualLock(pLink, m_growSize))
				++m_stats.m_locksSuccesses;
			m_stats.m_commitedSize = m_commitedSize += m_growSize;
#else
			VirtualLock(pLink, m_growSize);
			m_commitedSize += m_growSize;
#endif

			for(size_t i = 1; i < m_unitsPerGrow; ++i)
				pLink = pLink->m_pNext = (link*)((char*)pLink + m_allocSize);

			pLink->m_pNext = pOldFree;
		}
		else
		{
			DWORD dwErr = GetLastError();
			// It is very bad if we've got here ( out of memory )
		}
	}

	static void kill(char *p){ VirtualFree( p, 0, MEM_RELEASE ); }

    link*			m_pFree;
	size_t			m_allocSize;
    size_t			m_initSize;
	size_t			m_growSize;
	size_t			m_allocationGranularity;
	size_t			m_unitsPerGrow;
	size_t			m_commitedSize;

#ifdef ALLOCATOR_STATS
	MemoryPoolStatistics	m_stats;
#endif
    std::list<char*> m_pageList;
};
*/
template<typename T>
class FixedSizePoolAllocator;
// Specialization for void
template <> 
class FixedSizePoolAllocator<void>
{
public:
	typedef void* pointer;
	typedef const void* const_pointer;
	// reference to void members are impossible.
	typedef void value_type;

	template <class Other> struct rebind 
	{ 
		typedef FixedSizePoolAllocator<Other> other; 
	};
};    

template <typename T>
class FixedSizePoolAllocator
{
public:
	typedef size_t		size_type;
	typedef ptrdiff_t	difference_type;
	typedef T*			pointer;
	typedef const T*	const_pointer;
	typedef T&			reference;
	typedef const T&	const_reference;
	typedef T			value_type;

	template <class Other> struct rebind 
	{ 
		typedef FixedSizePoolAllocator<Other> other; 
	};

	FixedSizePoolAllocator() {}

    pointer			address( reference x )			const { return &x; }
    const_pointer	address( const_reference x )	const { return &x; }
    pointer			allocate( size_type size, FixedSizePoolAllocator<void>::const_pointer hint = 0 ) const
    {
        if( size == 1 ) 
		{
#ifdef NED_MALLOC
			return (T*)nedmalloc(sizeof(T));
#else
			return (T*)::operator new(sizeof(T));
#endif
//			return GetMemoryPool().Allocate();
		}
		else
		{
			// We shouldn't get here
//			assert( 0 );
			return NULL;
		}
    }

    template <class U> FixedSizePoolAllocator( const FixedSizePoolAllocator<U>& ) {}
    FixedSizePoolAllocator( const FixedSizePoolAllocator<T>& ) {}
    void deallocate( pointer p, size_type n ) const
    {
//		assert( 1 == n );
//        GetMemoryPool().Deallocate( p );
#ifdef NED_MALLOC
		nedfree(p);
#else
		::operator delete(p);
#endif
    }
    void deallocate( void *p, size_type n ) const
    {
//		assert( 1 == n );
//        GetMemoryPool().Deallocate( p );
#ifdef NED_MALLOC
		return nedfree(p);
#else
		::operator delete(p);
#endif
    }
    size_type max_size() const throw() { return size_t(-1) / sizeof( value_type ); }

    void construct(pointer p, const T& val)
    {
        new ((void*)p) T(val);
    }
    void construct( pointer p )
    {
        new ((void*)p) T();
    }
    
	void destroy( pointer p ) { p->T::~T(); }
/*
#ifdef ALLOCATOR_STATS
	// Statistics for memory pool
	static MemoryPoolStatistics& GetStatistics() { return GetMemoryPool().GetStatistics(); }
#endif
	FixedSizeMemoryPool<T>& GetMemoryPool() const
	{
	    static FixedSizeMemoryPool<T> s_memoryPool;
		return s_memoryPool;
	}
*/
};

//template <typename T> CFixedSizeMemoryPool<T> CFixedSizePoolAllocator<T>::s_memoryPool;

template <typename T, typename U>
inline bool operator==(const FixedSizePoolAllocator<T>&, const FixedSizePoolAllocator<U>){return true;}

template <typename T, typename U>
inline bool operator!=(const FixedSizePoolAllocator<T>&, const FixedSizePoolAllocator<U>){return false;}
